{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importación de librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchaudio\n",
    "import tarfile\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import IPython\n",
    "import gc\n",
    "import torch.nn.functional as F\n",
    "import torchaudio.transforms as tt\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import wandb\n",
    "import random\n",
    "import torch.optim as optim\n",
    "import torchaudio.transforms as T\n",
    "\n",
    "from torch import nn, optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, Subset, Dataset\n",
    "from typing import Dict  \n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from torch.utils.data import Dataset\n",
    "from torchaudio.datasets import GTZAN\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import random_split\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import collections\n",
    "from torchaudio.transforms import MelSpectrogram, AmplitudeToDB\n",
    "from torchaudio.sox_effects import apply_effects_tensor\n",
    "from typing import Optional\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Antes de los experimentos\n",
    "os.environ['WANDB_REPOSITORY'] = \"TP3_TD6\"\n",
    "os.environ['WANDB_BRANCH'] = \"main\"\n",
    "\n",
    "wandb.login(key = os.environ[\"WANDB_API_KEY\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir el sample rate\n",
    "samplerate = 22050"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalización de audio\n",
    "class NormalizeAudio(nn.Module):\n",
    "    def __init__(self, mean=0.0, std=1.0):\n",
    "        super(NormalizeAudio, self).__init__()\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "\n",
    "    def forward(self, waveform):\n",
    "        return (waveform - self.mean) / self.std\n",
    "\n",
    "# Transformación de TimeStretch en la forma de onda usando SoX\n",
    "class TimeStretchWaveform(nn.Module):\n",
    "    def __init__(self, rate: float = 1.2):\n",
    "        super(TimeStretchWaveform, self).__init__()\n",
    "        self.rate = rate\n",
    "\n",
    "    def forward(self, waveform: torch.Tensor, sample_rate: int = samplerate) -> torch.Tensor:\n",
    "        effects = [\n",
    "            ['tempo', f'{self.rate}']\n",
    "        ]\n",
    "        # Aplicar efectos de SoX\n",
    "        waveform, _ = apply_effects_tensor(waveform, sample_rate, effects)\n",
    "        return waveform\n",
    "\n",
    "# Transformación de PitchShift en la forma de onda usando SoX\n",
    "class PitchShiftWaveform(nn.Module):\n",
    "    def __init__(self, n_steps: int = 2):\n",
    "        super(PitchShiftWaveform, self).__init__()\n",
    "        self.n_steps = n_steps\n",
    "\n",
    "    def forward(self, waveform: torch.Tensor, sample_rate: int = samplerate) -> torch.Tensor:\n",
    "        effects = [\n",
    "            ['pitch', f'{self.n_steps * 100}'],  # Cada step corresponde a 100 cents\n",
    "            ['rate', f'{sample_rate}']  # Mantener el sample rate original\n",
    "        ]\n",
    "        # Aplicar efectos de SoX\n",
    "        waveform, _ = apply_effects_tensor(waveform, sample_rate, effects)\n",
    "        return waveform\n",
    "\n",
    "# Opcional: Resamplear si es necesario\n",
    "class ResampleWaveform(nn.Module):\n",
    "    def __init__(self, orig_freq: int, new_freq: int):\n",
    "        super(ResampleWaveform, self).__init__()\n",
    "        self.resample = torchaudio.transforms.Resample(orig_freq=orig_freq, new_freq=new_freq)\n",
    "\n",
    "    def forward(self, waveform: torch.Tensor) -> torch.Tensor:\n",
    "        return self.resample(waveform)\n",
    "\n",
    "class AddNoise(nn.Module):\n",
    "    def __init__(self, noise_level=0.005):\n",
    "        super(AddNoise, self).__init__()\n",
    "        self.noise_level = noise_level\n",
    "\n",
    "    def forward(self, spectrogram: torch.Tensor) -> torch.Tensor:\n",
    "        noise = torch.randn_like(spectrogram) * self.noise_level\n",
    "        return spectrogram + noise\n",
    "\n",
    "# Dataset\n",
    "class MusicDataset(Dataset):\n",
    "    def __init__(self, root='./genres_5sec', transform=None):\n",
    "        super().__init__()\n",
    "        self.root = root\n",
    "        self.files = []\n",
    "        self.classes = self.get_classes(self.root)\n",
    "        for c in self.classes:\n",
    "            class_dir = os.path.join(root, c)\n",
    "            for fname in os.listdir(class_dir):\n",
    "                if fname.endswith('.wav'):\n",
    "                    self.files.append((c, fname))\n",
    "        # Ordenar las clases para tener índices consistentes\n",
    "        self.classes = sorted(self.classes)\n",
    "        self.transform = transform \n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        genre, fname = self.files[idx]\n",
    "        fpath = os.path.join(self.root, genre, fname)\n",
    "        class_idx = self.classes.index(genre)\n",
    "        waveform, sample_rate = torchaudio.load(fpath)\n",
    "        \n",
    "        # Convertir a mono si tiene múltiples canales\n",
    "        if waveform.size(0) > 1:\n",
    "            waveform = torch.mean(waveform, dim=0, keepdim=True)\n",
    "        \n",
    "        # Resamplear si es necesario\n",
    "        if sample_rate != samplerate:\n",
    "            resample_transform = torchaudio.transforms.Resample(orig_freq=sample_rate, new_freq=samplerate)\n",
    "            waveform = resample_transform(waveform)\n",
    "            sample_rate = samplerate\n",
    "        \n",
    "        if self.transform:\n",
    "            waveform = self.transform(waveform)\n",
    "        \n",
    "        # Asegurar que la forma de onda tenga una longitud fija (ejemplo: 5 segundos)\n",
    "        fixed_length = samplerate * 5  # 5 segundos\n",
    "        if waveform.size(1) > fixed_length:\n",
    "            waveform = waveform[:, :fixed_length]\n",
    "        elif waveform.size(1) < fixed_length:\n",
    "            padding = fixed_length - waveform.size(1)\n",
    "            waveform = torch.nn.functional.pad(waveform, (0, padding))\n",
    "        \n",
    "        return waveform, class_idx\n",
    "        \n",
    "    def get_classes(self, root):\n",
    "        list_files = os.listdir(root)\n",
    "        classes = []\n",
    "        for file in list_files:\n",
    "            name = os.path.join(root, file)\n",
    "            if os.path.isdir(name):\n",
    "                classes.append(file)\n",
    "        return classes\n",
    "\n",
    "# Early Stopping\n",
    "class EarlyStopping:\n",
    "    \"\"\"\n",
    "    Early stopping para detener el entrenamiento cuando la pérdida de validación deja de mejorar.\n",
    "    \"\"\"\n",
    "    def __init__(self, patience=7, min_delta=0):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.best_loss = None\n",
    "        self.early_stop = False\n",
    "        \n",
    "    def __call__(self, val_loss):\n",
    "        if self.best_loss is None:\n",
    "            self.best_loss = val_loss\n",
    "        elif val_loss > self.best_loss - self.min_delta:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0\n",
    "\n",
    "# Modelos\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, n_input, nodes=128, n_output=10):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(n_input, nodes)\n",
    "        self.fc2 = nn.Linear(nodes, nodes)\n",
    "        self.fc3 = nn.Linear(nodes, n_output)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "class MLP2(nn.Module):\n",
    "    def __init__(self, n_input, nodes=128, n_output=10, hidden_layers=3):\n",
    "        super(MLP2, self).__init__()\n",
    "\n",
    "        self.dense_layers = nn.ModuleList()\n",
    "        in_features = n_input\n",
    "        \n",
    "        for i in range(hidden_layers):\n",
    "            layer = nn.Sequential(\n",
    "                nn.Linear(in_features, nodes),\n",
    "                nn.ReLU(),\n",
    "            )\n",
    "            self.dense_layers.append(layer)\n",
    "            in_features = nodes\n",
    "            \n",
    "        # Output layer\n",
    "        self.output = nn.Linear(nodes, n_output)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)  # Flatten input\n",
    "        \n",
    "        # Pass through all dense layers\n",
    "        for layer in self.dense_layers:\n",
    "            x = layer(x)\n",
    "            \n",
    "        x = self.output(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "    \n",
    "class CEL(nn.Module):\n",
    "    def __init__(self, n_input, nodes=128, n_output=10, hidden_layers=3, dropout_prob=0.3, dense_layers=2):\n",
    "        super(CEL, self).__init__()\n",
    "        \n",
    "        # Convolutional layers\n",
    "        self.conv_layers = nn.ModuleList()\n",
    "        in_channels = 1\n",
    "        out_channels = nodes\n",
    "        \n",
    "        for _ in range(hidden_layers):\n",
    "            conv_block = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=1, padding=1),\n",
    "                nn.BatchNorm2d(out_channels),\n",
    "                nn.ReLU(),\n",
    "                nn.MaxPool2d(kernel_size=2),\n",
    "                nn.Dropout2d(dropout_prob)\n",
    "            )\n",
    "            self.conv_layers.append(conv_block)\n",
    "            in_channels = out_channels\n",
    "            out_channels = out_channels * 2\n",
    "        \n",
    "        # Calculate the size after convolutions\n",
    "        self.flatten_size = nodes * (n_input // (2**hidden_layers)) * (n_input // (2**hidden_layers))\n",
    "        \n",
    "        # Dense layers\n",
    "        self.dense_layers = nn.ModuleList()\n",
    "        in_features = self.flatten_size\n",
    "        out_features = nodes\n",
    "        \n",
    "        for i in range(dense_layers):\n",
    "            if i == dense_layers - 1:\n",
    "                out_features = n_output\n",
    "            \n",
    "            dense_block = nn.Sequential(\n",
    "                nn.Linear(in_features, out_features),\n",
    "                nn.ReLU() if i < dense_layers - 1 else nn.Identity(),\n",
    "                nn.Dropout(dropout_prob) if i < dense_layers - 1 else nn.Identity()\n",
    "            )\n",
    "            self.dense_layers.append(dense_block)\n",
    "            in_features = out_features\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply convolutional layers\n",
    "        for conv_block in self.conv_layers:\n",
    "            x = conv_block(x)\n",
    "        \n",
    "        # Flatten\n",
    "        x = x.view(x.size(0), -1)\n",
    "        \n",
    "        # Apply dense layers\n",
    "        for dense_block in self.dense_layers:\n",
    "            x = dense_block(x)\n",
    "            \n",
    "        return F.log_softmax(x, dim=1)\n",
    "    \n",
    "class CEL2(nn.Module):\n",
    "    def __init__(self, n_input, nodes=128, n_output=10, hidden_layers=3, dropout_prob=0.3, dense_layers=2):\n",
    "        super(CEL2, self).__init__()\n",
    "        \n",
    "        # Convolutional layers\n",
    "        self.conv_layers = nn.ModuleList()\n",
    "        in_channels = 1\n",
    "        out_channels = nodes\n",
    "        \n",
    "        for _ in range(hidden_layers):\n",
    "            conv_block = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=1, padding=1),\n",
    "                nn.BatchNorm2d(out_channels),\n",
    "                nn.LeakyReLU(negative_slope=0.1),\n",
    "                nn.MaxPool2d(kernel_size=2),\n",
    "                nn.Dropout2d(dropout_prob)\n",
    "            )\n",
    "            self.conv_layers.append(conv_block)\n",
    "            in_channels = out_channels\n",
    "            out_channels = out_channels * 2\n",
    "        \n",
    "        # Calculate the size after convolutions\n",
    "        self.flatten_size = nodes * (n_input // (2**hidden_layers)) * (n_input // (2**hidden_layers))\n",
    "        \n",
    "        # Dense layers\n",
    "        self.dense_layers = nn.ModuleList()\n",
    "        in_features = self.flatten_size\n",
    "        out_features = nodes\n",
    "        \n",
    "        for i in range(dense_layers):\n",
    "            if i == dense_layers - 1:\n",
    "                out_features = n_output\n",
    "            \n",
    "            dense_block = nn.Sequential(\n",
    "                nn.Linear(in_features, out_features),\n",
    "                nn.LeakyReLU(negative_slope=0.1) if i < dense_layers - 1 else nn.Identity(),\n",
    "                nn.Dropout(dropout_prob) if i < dense_layers - 1 else nn.Identity()\n",
    "            )\n",
    "            self.dense_layers.append(dense_block)\n",
    "            in_features = out_features\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply convolutional layers\n",
    "        for conv_block in self.conv_layers:\n",
    "            x = conv_block(x)\n",
    "        \n",
    "        # Flatten\n",
    "        x = x.view(x.size(0), -1)\n",
    "        \n",
    "        # Apply dense layers\n",
    "        for dense_block in self.dense_layers:\n",
    "            x = dense_block(x)\n",
    "            \n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "class CEL3(nn.Module):\n",
    "    def __init__(self, n_input, nodes=128, n_output=10, hidden_layers=3, dropout_prob=0.3, dense_layers=2):\n",
    "        super(CEL3, self).__init__()\n",
    "        \n",
    "        # Convolutional layers\n",
    "        self.conv_layers = nn.ModuleList()\n",
    "        in_channels = 1\n",
    "        out_channels = nodes\n",
    "        \n",
    "        for _ in range(hidden_layers):\n",
    "            conv_block = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=1, padding=1),\n",
    "                nn.BatchNorm2d(out_channels),\n",
    "                nn.ELU(),\n",
    "                nn.MaxPool2d(kernel_size=2),\n",
    "                nn.Dropout2d(dropout_prob)\n",
    "            )\n",
    "            self.conv_layers.append(conv_block)\n",
    "            in_channels = out_channels\n",
    "            out_channels = out_channels * 2\n",
    "        \n",
    "        # Calculate the size after convolutions\n",
    "        self.flatten_size = nodes * (n_input // (2**hidden_layers)) * (n_input // (2**hidden_layers))\n",
    "        \n",
    "        # Dense layers\n",
    "        self.dense_layers = nn.ModuleList()\n",
    "        in_features = self.flatten_size\n",
    "        out_features = nodes\n",
    "        \n",
    "        for i in range(dense_layers):\n",
    "            if i == dense_layers - 1:\n",
    "                out_features = n_output\n",
    "            \n",
    "            dense_block = nn.Sequential(\n",
    "                nn.Linear(in_features, out_features),\n",
    "                nn.ELU() if i < dense_layers - 1 else nn.Identity(),\n",
    "                nn.Dropout(dropout_prob) if i < dense_layers - 1 else nn.Identity()\n",
    "            )\n",
    "            self.dense_layers.append(dense_block)\n",
    "            in_features = out_features\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply convolutional layers\n",
    "        for conv_block in self.conv_layers:\n",
    "            x = conv_block(x)\n",
    "        \n",
    "        # Flatten\n",
    "        x = x.view(x.size(0), -1)\n",
    "        \n",
    "        # Apply dense layers\n",
    "        for dense_block in self.dense_layers:\n",
    "            x = dense_block(x)\n",
    "            \n",
    "        return F.log_softmax(x, dim=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parseo de géneros\n",
    "def parse_genres(fname):\n",
    "    \"\"\"\n",
    "    Extrae el nombre del género musical a partir del nombre del archivo.\n",
    "\n",
    "    Args:\n",
    "        fname (str): Ruta completa del archivo.\n",
    "\n",
    "    Returns:\n",
    "        str: Nombre del género extraído del archivo.\n",
    "    \"\"\"\n",
    "    parts = fname.split('/')[-1].split('.')[0]\n",
    "    return parts #' '.join(parts[0])\n",
    "\n",
    "# Obtener clases\n",
    "def classes(root = './genres_5sec'):\n",
    "    \"\"\"\n",
    "    Lista todas las clases de géneros musicales presentes en el directorio especificado.\n",
    "\n",
    "    Args:\n",
    "        root (str, opcional): Ruta al directorio raíz que contiene las carpetas de géneros. Por defecto es './genres_5sec'.\n",
    "\n",
    "    Returns:\n",
    "        list: Lista de nombres de clases (géneros musicales).\n",
    "    \"\"\"\n",
    "    list_files = os.listdir(root)\n",
    "    classes = []\n",
    "    for file in list_files:\n",
    "        name = f'{root}/{file}'\n",
    "        if os.path.isdir(name):\n",
    "            classes.append(file)\n",
    "    return classes\n",
    "\n",
    "# Plotear waveforms\n",
    "def plot_waveforms(dataset, num_samples=5, figsize=(15, 10)):\n",
    "    \"\"\"\n",
    "    Plotea múltiples waveforms con sus etiquetas correspondientes.\n",
    "    \n",
    "    Args:\n",
    "        dataset: Dataset que contiene los audios y etiquetas.\n",
    "        num_samples (int, opcional): Número de waveforms a mostrar. Por defecto es 5.\n",
    "        figsize (tuple, opcional): Tamaño de la figura (ancho, alto). Por defecto es (15, 10).\n",
    "    \"\"\"\n",
    "    \n",
    "    fig, axes = plt.subplots(num_samples, 1, figsize=figsize)\n",
    "    fig.suptitle('Waveforms de diferentes géneros musicales', fontsize=16)\n",
    "    \n",
    "    # Asegurarse de que axes sea siempre una lista\n",
    "    if num_samples == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    # Obtener índices aleatorios sin repetición\n",
    "    indices = random.sample(range(len(dataset)), num_samples)\n",
    "    \n",
    "    for idx, ax in zip(indices, axes):\n",
    "        waveform, label = dataset[idx]\n",
    "        genre = dataset.classes[label]\n",
    "        \n",
    "        # Plotear la waveform\n",
    "        ax.plot(waveform.t().numpy()[0], linewidth=1)\n",
    "        ax.set_title(f'Género: {genre}')\n",
    "        ax.set_xlabel('Tiempo (muestras)')\n",
    "        ax.set_ylabel('Amplitud')\n",
    "        ax.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Para comparar waveforms del mismo género\n",
    "def plot_genre_waveforms(dataset, genre_name, num_samples=3, figsize=(15, 8)):\n",
    "    \"\"\"\n",
    "    Plotea múltiples waveforms del mismo género.\n",
    "    \n",
    "    Args:\n",
    "        dataset: Dataset que contiene los audios y etiquetas.\n",
    "        genre_name (str): Nombre del género a visualizar.\n",
    "        num_samples (int, opcional): Número de waveforms a mostrar. Por defecto es 3.\n",
    "        figsize (tuple, opcional): Tamaño de la figura (ancho, alto). Por defecto es (15, 8).\n",
    "    \"\"\"\n",
    "    \n",
    "    genre_idx = dataset.classes.index(genre_name)\n",
    "    genre_samples = [(i, wave, label) for i, (wave, label) in enumerate(dataset) if label == genre_idx]\n",
    "    \n",
    "    if len(genre_samples) < num_samples:\n",
    "        num_samples = len(genre_samples)\n",
    "    \n",
    "    selected_samples = random.sample(genre_samples, num_samples)\n",
    "    \n",
    "    fig, axes = plt.subplots(num_samples, 1, figsize=figsize)\n",
    "    fig.suptitle(f'Waveforms del género: {genre_name}', fontsize=16)\n",
    "    \n",
    "    if num_samples == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    for i, (idx, waveform, _) in enumerate(selected_samples):\n",
    "        axes[i].plot(waveform.t().numpy()[0], linewidth=1)\n",
    "        axes[i].set_title(f'Muestra {idx}')\n",
    "        axes[i].set_xlabel('Tiempo (muestras)')\n",
    "        axes[i].set_ylabel('Amplitud')\n",
    "        axes[i].grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Stratify split\n",
    "def stratify_split(dataset: Dataset, classes: list[int], transforms: Dict[str, nn.Module] = {'train': None, 'val': None, 'test': None}) -> tuple[Subset, Subset, Subset]:\n",
    "    \"\"\"\n",
    "    Divide el dataset en conjuntos de entrenamiento, validación y prueba de manera estratificada.\n",
    "\n",
    "    Args:\n",
    "        dataset (Dataset): Dataset completo que contiene los datos y etiquetas.\n",
    "        classes (list[int]): Lista de índices de clases presentes en el dataset.\n",
    "        transforms (Dict[str, nn.Module], opcional): Transformaciones a aplicar a cada subconjunto. Por defecto es {'train': None, 'val': None, 'test': None}.\n",
    "\n",
    "    Returns:\n",
    "        tuple[Subset, Subset, Subset]: Tupla que contiene los subconjuntos de entrenamiento, validación y prueba.\n",
    "    \"\"\"\n",
    "    \n",
    "    train_indices = []\n",
    "    val_indices = []\n",
    "    test_indices = []\n",
    "\n",
    "    NUM_CLASSES = len(classes)\n",
    "    total_samples = len(dataset)\n",
    "    \n",
    "    # Calcular el número de muestras para cada conjunto\n",
    "    train_samples = int(total_samples * 0.8)\n",
    "    val_samples = int(total_samples * 0.1)\n",
    "    test_samples = total_samples - train_samples - val_samples  # Resto para test\n",
    "\n",
    "    train_samples_per_class = train_samples // NUM_CLASSES\n",
    "    val_samples_per_class = val_samples // NUM_CLASSES\n",
    "    test_samples_per_class = test_samples // NUM_CLASSES\n",
    "\n",
    "    train_target_counter = collections.Counter()\n",
    "    val_target_counter = collections.Counter()\n",
    "    test_target_counter = collections.Counter()\n",
    "\n",
    "    for idx, data in enumerate(dataset):\n",
    "        target = data[1]\n",
    "        if train_target_counter[target] < train_samples_per_class:\n",
    "            train_indices.append(idx)\n",
    "            train_target_counter[target] += 1\n",
    "        elif val_target_counter[target] < val_samples_per_class:\n",
    "            val_target_counter[target] += 1\n",
    "            val_indices.append(idx)\n",
    "        elif test_target_counter[target] < test_samples_per_class:\n",
    "            test_target_counter[target] += 1\n",
    "            test_indices.append(idx)\n",
    "\n",
    "    train_dataset = Subset(\n",
    "        MusicDataset(root=dataset.root, transform=transforms['train']), \n",
    "        train_indices\n",
    "    )\n",
    "    \n",
    "    val_dataset = Subset(\n",
    "        MusicDataset(root=dataset.root, transform=transforms['val']), \n",
    "        val_indices\n",
    "    )\n",
    "\n",
    "    test_dataset = Subset(\n",
    "        MusicDataset(root=dataset.root, transform=transforms['test']), \n",
    "        test_indices\n",
    "    )\n",
    "\n",
    "    return train_dataset, val_dataset, test_dataset\n",
    "\n",
    "def count_parameters(model):\n",
    "    \"\"\"\n",
    "    Cuenta el número de parámetros entrenables en un modelo.\n",
    "\n",
    "    Args:\n",
    "        model: Modelo de PyTorch cuyo número de parámetros se desea contar.\n",
    "\n",
    "    Returns:\n",
    "        int: Número total de parámetros entrenables.\n",
    "    \"\"\"\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "# Configuración para probar diferentes optimizadores\n",
    "def get_optimizer(optimizer_name, model, lr):\n",
    "    \"\"\"\n",
    "    Retorna el optimizador de PyTorch basado en el nombre proporcionado.\n",
    "\n",
    "    Args:\n",
    "        optimizer_name (str): Nombre del optimizador ('adam', 'adamw', 'sgd', etc.).\n",
    "        model: Modelo de PyTorch cuyos parámetros serán optimizados.\n",
    "        lr (float): Tasa de aprendizaje para el optimizador.\n",
    "\n",
    "    Returns:\n",
    "        torch.optim.Optimizer: Instancia del optimizador seleccionado.\n",
    "    \"\"\"\n",
    "    if optimizer_name == 'adam':\n",
    "        return torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    elif optimizer_name == 'adamw':\n",
    "        return torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=0.01)\n",
    "    elif optimizer_name == 'sgd':\n",
    "        return torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
    "    else:\n",
    "        return torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "def get_scheduler(scheduler_name, optimizer, num_epochs, trainloader):\n",
    "    \"\"\"\n",
    "    Retorna el scheduler de PyTorch basado en el nombre proporcionado.\n",
    "\n",
    "    Args:\n",
    "        scheduler_name (str): Nombre del scheduler ('plateau', 'cosine', 'step', 'onecycle', 'warmrestarts', etc.).\n",
    "        optimizer (torch.optim.Optimizer): Optimizador para el cual se aplicará el scheduler.\n",
    "        num_epochs (int): Número total de épocas de entrenamiento.\n",
    "        trainloader (DataLoader): DataLoader del conjunto de entrenamiento.\n",
    "\n",
    "    Returns:\n",
    "        torch.optim.lr_scheduler._LRScheduler o None: Instancia del scheduler seleccionado o None si no se especifica.\n",
    "    \"\"\"\n",
    "    if scheduler_name == 'plateau':\n",
    "        return torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            optimizer, mode='min', factor=0.5, patience=3, verbose=False\n",
    "        )\n",
    "    elif scheduler_name == 'cosine':\n",
    "        return torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "            optimizer, T_max=num_epochs, eta_min=1e-6\n",
    "        )\n",
    "    elif scheduler_name == 'step':\n",
    "        return torch.optim.lr_scheduler.StepLR(\n",
    "            optimizer, step_size=30, gamma=0.1\n",
    "        )\n",
    "    elif scheduler_name == 'onecycle':\n",
    "        return torch.optim.lr_scheduler.OneCycleLR(\n",
    "            optimizer,\n",
    "            max_lr=0.01,\n",
    "            epochs=num_epochs,\n",
    "            steps_per_epoch=len(trainloader)\n",
    "        )\n",
    "    elif scheduler_name == 'warmrestarts':\n",
    "        return torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
    "            optimizer, T_0=10, T_mult=2\n",
    "        )\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def l1_regularization(model, lambda_l1=0.01):\n",
    "    \"\"\"\n",
    "    Aplica regularización L1 al modelo.\n",
    "    \n",
    "    Args:\n",
    "        model: Modelo PyTorch\n",
    "        lambda_l1: Factor de regularización L1\n",
    "    \n",
    "    Returns:\n",
    "        torch.Tensor: Término de regularización L1\n",
    "    \"\"\"\n",
    "    l1_reg = torch.tensor(0., requires_grad=True)\n",
    "    for param in model.parameters():\n",
    "        l1_reg = l1_reg + torch.norm(param, 1)\n",
    "    return lambda_l1 * l1_reg\n",
    "\n",
    "def l2_regularization(model, lambda_l2=0.01):\n",
    "    \"\"\"\n",
    "    Aplica regularización L2 al modelo.\n",
    "    \n",
    "    Args:\n",
    "        model: Modelo PyTorch\n",
    "        lambda_l2: Factor de regularización L2\n",
    "    \n",
    "    Returns:\n",
    "        torch.Tensor: Término de regularización L2\n",
    "    \"\"\"\n",
    "    l2_reg = torch.tensor(0., requires_grad=True)\n",
    "    for param in model.parameters():\n",
    "        l2_reg = l2_reg + torch.norm(param, 2)\n",
    "    return lambda_l2 * l2_reg\n",
    "\n",
    "def train_model(model_name, device, hyperparameter_configs, dataset, train_dataset, val_dataset, n_input, by, \n",
    "                scheduler_name='None', optimizer_name='adam', regularization_type=None, lambda_reg=0.01, early_stopping_patience=None):\n",
    "    \"\"\"\n",
    "    Entrena el modelo especificado con las configuraciones de hiperparámetros proporcionadas.\n",
    "\n",
    "    Args:\n",
    "        model_name (str): Nombre del modelo a entrenar ('MLP2', 'CEL', 'CEL2', 'CEL3', 'MLP', etc.).\n",
    "        num_epochs (int): Número de épocas para el entrenamiento.\n",
    "        device (torch.device): Dispositivo donde se realizará el entrenamiento (CPU o GPU).\n",
    "        hyperparameter_configs (list of dict): Lista de configuraciones de hiperparámetros para probar.\n",
    "        dataset: Dataset completo que contiene los datos y etiquetas.\n",
    "        train_dataset (Subset): Subconjunto de entrenamiento.\n",
    "        val_dataset (Subset): Subconjunto de validación.\n",
    "        n_input (int): Tamaño de la entrada para el modelo.\n",
    "        by (tuple): Información adicional para nombrar el experimento.\n",
    "        scheduler_name (str, opcional): Nombre del scheduler a utilizar. Por defecto es 'None'.\n",
    "        optimizer_name (str, opcional): Nombre del optimizador a utilizar. Por defecto es 'adam'.\n",
    "        regularization_type: Tipo de regularización ('l1', 'l2', None)\n",
    "        lambda_reg: Factor de regularización para L1/L2\n",
    "        early_stopping_patience: Número de épocas para early stopping\n",
    "    Retorna:\n",
    "        None\n",
    "    \"\"\"\n",
    "    \n",
    "    # Inicializar variables para seguimiento de mejores y peores experimentos\n",
    "    best_valid_loss = float('inf')\n",
    "\n",
    "    # Definir función de pérdida\n",
    "    loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Inicializar early stopping si se especifica\n",
    "    early_stopping = None\n",
    "    if early_stopping_patience:\n",
    "        early_stopping = EarlyStopping(patience=early_stopping_patience)\n",
    "\n",
    "    for config in hyperparameter_configs:\n",
    "        \n",
    "        num_epochs = config['epochs']\n",
    "\n",
    "        reg_suffix = f\"_reg_{regularization_type}\" if regularization_type else \"\"\n",
    "\n",
    "        if model_name == None:\n",
    "            return 'Especificar modelo a usar'\n",
    "        \n",
    "        if model_name == 'MLP2':\n",
    "            experiment_name = f\"{by[0]}_{model_name}_epochs_{num_epochs}_opt_{optimizer_name}_scheduler_{scheduler_name}_lr_{config['learning_rate']}_nodes_{config['nodes']}_hl_{config['hidden_layers']}{reg_suffix}\" \n",
    "        \n",
    "        elif model_name == 'CEL' or model_name == 'CEL2' or model_name == 'CEL3':\n",
    "            experiment_name = f\"{by[0]}_{model_name}_epochs_{num_epochs}_opt_{optimizer_name}_scheduler_{scheduler_name}_lr_{config['learning_rate']}_nodes_{config['nodes']}_hl_{config['hidden_layers']}_dp_{config['dropout_prob']}_dl_{config['dense_layers']}{reg_suffix}\"\n",
    "        \n",
    "        elif model_name == 'MLP':\n",
    "            experiment_name = f\"{by[0]}_{model_name}_epochs_{num_epochs}_opt_{optimizer_name}_scheduler_{scheduler_name}_lr_{config['learning_rate']}_nodes_{config['nodes']}{reg_suffix}\"\n",
    "        \n",
    "        wandb.init(\n",
    "            project='Trabajo_Practico_3_Music_Genre_Classification',\n",
    "            name=experiment_name,\n",
    "            reinit=True,\n",
    "            save_code=True,\n",
    "            config={\n",
    "                \"architecture\": \"MLP2\",\n",
    "                \"dataset\": \"GTZAN\",\n",
    "                \"num_classes\": len(dataset.classes),\n",
    "                \"classes\": dataset.classes,\n",
    "                \"sample_rate\": samplerate,\n",
    "                \"input_size\": n_input,\n",
    "                **config\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        # Crear DataLoader con el batch_size actual\n",
    "        trainloader = DataLoader(train_dataset, batch_size=config['batch_size'], shuffle=True)\n",
    "        valloader = DataLoader(val_dataset, batch_size=config['batch_size'], shuffle=False)\n",
    "        \n",
    "        # Inicializar modelo y optimizador con la configuración actual\n",
    "        if model_name == \"MLP2\":\n",
    "            model = MLP2(\n",
    "                n_input=n_input,\n",
    "                nodes=config['nodes'],\n",
    "                n_output=len(dataset.classes),\n",
    "                hidden_layers=config['hidden_layers']\n",
    "            ).to(device)\n",
    "\n",
    "        if model_name == \"CEL\" or model_name == \"CEL2\" or model_name == \"CEL3\":\n",
    "            model = CEL(\n",
    "                nn_input=n_input,\n",
    "                nodes=config['nodes'],\n",
    "                n_output=len(dataset.classes),\n",
    "                hidden_layers=config['hidden_layers'],\n",
    "                dropout_prob=config.get('dropout_prob', 0.3),\n",
    "                dense_layers=config.get('dense_layers', 2)\n",
    "            ).to(device)\n",
    "\n",
    "        if model_name == 'MLP':\n",
    "            model = MLP(\n",
    "                n_input=n_input,\n",
    "                nodes=config['nodes'],\n",
    "                n_output=len(dataset.classes)\n",
    "            ).to(device)\n",
    "\n",
    "        optimizer = get_optimizer(optimizer_name, model, config['learning_rate'])\n",
    "        \n",
    "        if len(scheduler_name) > 0: \n",
    "            scheduler = get_scheduler(scheduler_name, optimizer, num_epochs, trainloader)\n",
    "        \n",
    "        print(f\"\\nStarting experiment: {experiment_name}\")\n",
    "        print(model)\n",
    "        print(f\"Number of parameters: {count_parameters(model)}\")\n",
    "        \n",
    "        # Bucle de entrenamiento\n",
    "        for epoch in range(num_epochs):\n",
    "            model.train()\n",
    "            train_losses = []\n",
    "            \n",
    "            for wav, genre_index in trainloader:\n",
    "                wav = wav.to(device)\n",
    "                genre_index = genre_index.to(device)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                # Reshape input: [batch_size, channels, height, width] si es necesario\n",
    "                wav = wav.unsqueeze(1)  # Añadir dimensión de canal si es necesario\n",
    "                out = model(wav.view(wav.size(0), -1))  # Aplanar el input\n",
    "                loss = loss_function(out, genre_index)\n",
    "\n",
    "                # Aplicar regularización si está especificada\n",
    "                if regularization_type == 'l1':\n",
    "                    loss += l1_regularization(model, lambda_reg)\n",
    "                elif regularization_type == 'l2':\n",
    "                    loss += l2_regularization(model, lambda_reg)\n",
    "                \n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                train_losses.append(loss.item())\n",
    "            \n",
    "            # Validación\n",
    "            model.eval()\n",
    "            valid_losses = []\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for wav, genre_index in valloader:\n",
    "                    wav = wav.to(device)\n",
    "                    genre_index = genre_index.to(device)\n",
    "                    \n",
    "                    wav = wav.unsqueeze(1)  # Añadir dimensión de canal si es necesario\n",
    "                    out = model(wav.view(wav.size(0), -1))\n",
    "                    loss = loss_function(out, genre_index)\n",
    "                    valid_losses.append(loss.item())\n",
    "                    \n",
    "                    _, predicted = torch.max(out.data, 1)\n",
    "                    total += genre_index.size(0)\n",
    "                    correct += (predicted == genre_index).sum().item()\n",
    "            \n",
    "            train_loss = np.mean(train_losses)\n",
    "            valid_loss = np.mean(valid_losses)\n",
    "            accuracy = correct / total\n",
    "            \n",
    "            # Logging en wandb\n",
    "            wandb.log({\n",
    "                \"train_loss\": train_loss,\n",
    "                \"valid_loss\": valid_loss,\n",
    "                \"accuracy\": accuracy,\n",
    "                \"epoch\": epoch\n",
    "            })\n",
    "            \n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}] - '\n",
    "                f'Train Loss: {train_loss:.4f}, '\n",
    "                f'Valid Loss: {valid_loss:.4f}, '\n",
    "                f'Accuracy: {accuracy:.4f}')\n",
    "            \n",
    "            if early_stopping:\n",
    "                early_stopping(valid_loss)\n",
    "                if early_stopping.early_stop:\n",
    "                    print(f\"Early stopping triggered at epoch {epoch+1}\")\n",
    "                    break\n",
    "\n",
    "            # Actualizar scheduler\n",
    "            if scheduler is not None:\n",
    "                scheduler.step(valid_loss)\n",
    "            \n",
    "            if not os.path.exists('best_models'):\n",
    "                os.makedirs('best_models')\n",
    "\n",
    "            # Guardar el mejor modelo\n",
    "            if valid_loss < best_valid_loss:\n",
    "                best_valid_loss = valid_loss\n",
    "                best_experiment = config\n",
    "                torch.save(model.state_dict(), f'best_models/{experiment_name}.pt')\n",
    "                print(f'Saved best model at epoch {epoch+1}')\n",
    "\n",
    "        # Finalizar wandb\n",
    "        wandb.finish()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP3: Encodeador de música"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed = 43992294\n",
    "batch_size = 20\n",
    "\n",
    "torch.manual_seed(random_seed);\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformaciones para entrenamiento de la forma de onda\n",
    "waveform_train_transform = transforms.Compose([\n",
    "    NormalizeAudio(mean=0.0, std=1.0),\n",
    "    TimeStretchWaveform(rate=1.2),\n",
    "    PitchShiftWaveform(n_steps=2),\n",
    "])\n",
    "\n",
    "# Transformaciones para validación y prueba de la forma de onda\n",
    "waveform_val_test_transform = transforms.Compose([\n",
    "    NormalizeAudio(mean=0.0, std=1.0),\n",
    "])\n",
    "\n",
    "# Transformaciones para entrenamiento del espectrograma\n",
    "spectrogram_train_transform = transforms.Compose([\n",
    "    NormalizeAudio(mean=0.0, std=1.0),\n",
    "    T.FrequencyMasking(freq_mask_param=30),  \n",
    "    T.TimeMasking(time_mask_param=40),          \n",
    "    AddNoise(noise_level=0.005),\n",
    "])\n",
    "\n",
    "# Transformaciones para validación y prueba del espectrograma\n",
    "spectrogram_val_test_transform = transforms.Compose([\n",
    "    NormalizeAudio(mean=0.0, std=1.0),\n",
    "])\n",
    "\n",
    "transforms_waveform = {\n",
    "    'train': waveform_train_transform,\n",
    "    'val': waveform_val_test_transform,\n",
    "    'test': waveform_val_test_transform\n",
    "}\n",
    "\n",
    "transforms_spectrogram = {\n",
    "    'train': spectrogram_train_transform,\n",
    "    'val': spectrogram_val_test_transform,\n",
    "    'test': spectrogram_val_test_transform\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else \n",
    "                      'mps' if torch.backends.mps.is_available() else \n",
    "                      'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualización de los archivos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = MusicDataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividir el dataset usando dataset.classes en lugar de la función classes\n",
    "train_dataset, val_dataset, test_dataset = stratify_split(\n",
    "    dataset, \n",
    "    dataset.classes,\n",
    "    transforms_spectrogram\n",
    ")\n",
    "\n",
    "# Crear los dataloaders\n",
    "trainloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "valloader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "testloader = DataLoader(test_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(trainloader), len(valloader), len(testloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_dataset), len(val_dataset), len(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "waveform,label= train_dataset[789]\n",
    "print(\"shape of waveform {}, sample rate with {}, label is {} \".format(waveform.size(),samplerate,label))\n",
    "\n",
    "waveform,label= val_dataset[60]\n",
    "print(\"shape of waveform {}, sample rate with {}, label is {} \".format(waveform.size(),samplerate,label))\n",
    "\n",
    "waveform,label= test_dataset[70]\n",
    "print(\"shape of waveform {}, sample rate with {}, label is {} \".format(waveform.size(),samplerate,label))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "specgram=tt.Spectrogram()(waveform)\n",
    "print(\"shape of spectogram {}\".format(specgram.size()))\n",
    "\n",
    "plt.figure(figsize=(20, 5))\n",
    "plt.imshow(specgram.log2()[0,:,:].numpy(),cmap='magma')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Waveform: {}\\n\".format(waveform))\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(waveform.t().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IPython.display.Audio(waveform,rate=samplerate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clasificación **(Prueba de modelos)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = MusicDataset()\n",
    "train_dataset, val_dataset, test_dataset = stratify_split(\n",
    "    dataset, \n",
    "    dataset.classes,\n",
    "    transforms_waveform\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_input = 110250"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameter_configs_MLP = [\n",
    "    {\n",
    "        \"learning_rate\": 0.001,\n",
    "        \"batch_size\": 32,\n",
    "        \"nodes\": 64,\n",
    "        \"epochs\": 100\n",
    "    },\n",
    "    {\n",
    "        \"learning_rate\": 0.0005,\n",
    "        \"batch_size\": 32,\n",
    "        \"nodes\": 128,\n",
    "        \"epochs\": 300\n",
    "    },\n",
    "    {\n",
    "        \"learning_rate\": 0.001,\n",
    "        \"batch_size\": 16,\n",
    "        \"nodes\": 32,\n",
    "        \"epochs\": 300\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### by Waveform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limpiar caché de CUDA y recolectar basura\n",
    "torch.cuda.empty_cache()\n",
    "del dataset, train_dataset, val_dataset, test_dataset\n",
    "gc.collect()  # Importante para liberar memoria RAM\n",
    "\n",
    "dataset = MusicDataset()\n",
    "train_dataset, val_dataset, test_dataset = stratify_split(\n",
    "    dataset, \n",
    "    dataset.classes,\n",
    "    transforms_waveform\n",
    ")\n",
    "\n",
    "by = \"waveform\"\n",
    "model_name = \"MLP\"\n",
    "train_model(model_name, device, hyperparameter_configs_MLP, dataset, train_dataset, val_dataset, n_input, by)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### by Spectogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limpiar caché de CUDA y recolectar basura\n",
    "torch.cuda.empty_cache()\n",
    "del dataset, train_dataset, val_dataset, test_dataset, trainloader, valloader, testloader\n",
    "gc.collect()  # Importante para liberar memoria RAM\n",
    "\n",
    "dataset = MusicDataset()\n",
    "train_dataset, val_dataset, test_dataset = stratify_split(\n",
    "    dataset, \n",
    "    dataset.classes,\n",
    "    transforms_waveform\n",
    ")\n",
    "\n",
    "by = \"spectogram\"\n",
    "model_name = \"MLP\"\n",
    "epochs = [50,100,300]\n",
    "for i in range(len(epochs)):\n",
    "    train_model(model_name, epochs[i], device, hyperparameter_configs_MLP, dataset, train_dataset, val_dataset, n_input, by)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define una lista de configuraciones de hiperparámetros sin 'dense_layers'\n",
    "hyperparameter_configs_MLP2 = [\n",
    "    {\n",
    "        \"learning_rate\": 0.001,\n",
    "        \"batch_size\": 32,\n",
    "        \"nodes\": 64,\n",
    "        \"hidden_layers\": 3\n",
    "    },\n",
    "    {\n",
    "        \"learning_rate\": 0.0005,\n",
    "        \"batch_size\": 32,\n",
    "        \"nodes\": 128,\n",
    "        \"hidden_layers\": 4\n",
    "    },\n",
    "    {\n",
    "        \"learning_rate\": 0.001,\n",
    "        \"batch_size\": 16,\n",
    "        \"nodes\": 32,\n",
    "        \"hidden_layers\": 2\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### by Waveform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limpiar caché de CUDA y recolectar basura\n",
    "torch.cuda.empty_cache()\n",
    "del dataset, train_dataset, val_dataset, test_dataset, trainloader, valloader, testloader\n",
    "gc.collect()  # Importante para liberar memoria RAM\n",
    "\n",
    "dataset = MusicDataset()\n",
    "train_dataset, val_dataset, test_dataset = stratify_split(\n",
    "    dataset, \n",
    "    dataset.classes,\n",
    "    transforms_waveform\n",
    ")\n",
    "\n",
    "by = \"waveform\"\n",
    "model_name = \"MLP2\"\n",
    "epochs = [50,100,300]\n",
    "for i in range(len(epochs)):\n",
    "    train_model(model_name, epochs[i], device, hyperparameter_configs_MLP2, dataset, train_dataset, val_dataset, n_input, by)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### by Spectogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limpiar caché de CUDA y recolectar basura\n",
    "torch.cuda.empty_cache()\n",
    "del dataset, train_dataset, val_dataset, test_dataset, trainloader, valloader, testloader\n",
    "gc.collect()  # Importante para liberar memoria RAM\n",
    "\n",
    "dataset = MusicDataset()\n",
    "train_dataset, val_dataset, test_dataset = stratify_split(\n",
    "    dataset, \n",
    "    dataset.classes,\n",
    "    transforms_waveform\n",
    ")\n",
    "\n",
    "by = \"spectogram\"\n",
    "model_name = \"MLP2\"\n",
    "epochs = [50,100,300]\n",
    "for i in range(len(epochs)):\n",
    "    train_model(model_name, epochs[i], device, hyperparameter_configs_MLP2, dataset, train_dataset, val_dataset, n_input, by)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a list of hyperparameter configurations\n",
    "hyperparameter_configs_CEL = [\n",
    "    {\n",
    "        \"learning_rate\": 0.001,\n",
    "        \"batch_size\": 32,\n",
    "        \"nodes\": 64,\n",
    "        \"hidden_layers\": 3,\n",
    "        \"dropout_prob\": 0.3,\n",
    "        \"dense_layers\": 2\n",
    "    },\n",
    "    {\n",
    "        \"learning_rate\": 0.0005,\n",
    "        \"batch_size\": 32,\n",
    "        \"nodes\": 128,\n",
    "        \"hidden_layers\": 4,\n",
    "        \"dropout_prob\": 0.4,\n",
    "        \"dense_layers\": 2\n",
    "    },\n",
    "    {\n",
    "        \"learning_rate\": 0.001,\n",
    "        \"batch_size\": 16,\n",
    "        \"nodes\": 32,\n",
    "        \"hidden_layers\": 2,\n",
    "        \"dropout_prob\": 0.2,\n",
    "        \"dense_layers\": 3\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### by Waveform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limpiar caché de CUDA y recolectar basura\n",
    "torch.cuda.empty_cache()\n",
    "del dataset, train_dataset, val_dataset, test_dataset, trainloader, valloader, testloader\n",
    "gc.collect()  # Importante para liberar memoria RAM\n",
    "\n",
    "dataset = MusicDataset()\n",
    "train_dataset, val_dataset, test_dataset = stratify_split(\n",
    "    dataset, \n",
    "    dataset.classes,\n",
    "    transforms_waveform\n",
    ")\n",
    "\n",
    "by = \"waveform\"\n",
    "model_name = \"CEL\"\n",
    "epochs = [50,100,300]   \n",
    "for i in range(len(epochs)):\n",
    "    train_model(model_name, epochs[i], device, hyperparameter_configs_CEL, dataset, train_dataset, val_dataset, n_input, by)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### by Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limpiar caché de CUDA y recolectar basura\n",
    "torch.cuda.empty_cache()\n",
    "del dataset, train_dataset, val_dataset, test_dataset, trainloader, valloader, testloader\n",
    "gc.collect()  # Importante para liberar memoria RAM\n",
    "\n",
    "dataset = MusicDataset()\n",
    "train_dataset, val_dataset, test_dataset = stratify_split(\n",
    "    dataset, \n",
    "    dataset.classes,\n",
    "    transforms_waveform\n",
    ")\n",
    "\n",
    "by = \"waveform\"\n",
    "model_name = \"CEL\"\n",
    "num_epochs = 300  \n",
    "optimizer_names = ['adam', 'sgd', 'rmsprop']\n",
    "for i in range(len(optimizer_names)):\n",
    "    train_model(model_name, num_epochs, device, hyperparameter_configs_CEL, dataset, train_dataset, val_dataset, n_input, by, optimizer_name=optimizer_names[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### by Spectogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limpiar caché de CUDA y recolectar basura\n",
    "torch.cuda.empty_cache()\n",
    "del dataset, train_dataset, val_dataset, test_dataset, trainloader, valloader, testloader\n",
    "gc.collect()  # Importante para liberar memoria RAM\n",
    "\n",
    "dataset = MusicDataset()\n",
    "train_dataset, val_dataset, test_dataset = stratify_split(\n",
    "    dataset, \n",
    "    dataset.classes,\n",
    "    transforms_waveform\n",
    ")\n",
    "\n",
    "by = \"spectogram\"\n",
    "model_name = \"CEL\"\n",
    "epochs = [50,100,300]   \n",
    "for i in range(len(epochs)):\n",
    "    train_model(model_name, epochs[i], device, hyperparameter_configs_CEL, dataset, train_dataset, val_dataset, n_input, by)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### by Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limpiar caché de CUDA y recolectar basura\n",
    "torch.cuda.empty_cache()\n",
    "del dataset, train_dataset, val_dataset, test_dataset, trainloader, valloader, testloader\n",
    "gc.collect()  # Importante para liberar memoria RAM\n",
    "\n",
    "dataset = MusicDataset()\n",
    "train_dataset, val_dataset, test_dataset = stratify_split(\n",
    "    dataset, \n",
    "    dataset.classes,\n",
    "    transforms_spectrogram\n",
    ")\n",
    "\n",
    "by = \"spectogram\"\n",
    "model_name = \"CEL\"\n",
    "num_epochs = 300  \n",
    "optimizer_names = ['adam', 'sgd', 'rmsprop']\n",
    "for i in range(len(optimizer_names)):\n",
    "    train_model(model_name, num_epochs, device, hyperparameter_configs_CEL, dataset, train_dataset, val_dataset, n_input, by, optimizer_name=optimizer_names[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CEL2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### by Waveform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limpiar caché de CUDA y recolectar basura\n",
    "torch.cuda.empty_cache()\n",
    "del dataset, train_dataset, val_dataset, test_dataset, trainloader, valloader, testloader\n",
    "gc.collect()  # Importante para liberar memoria RAM\n",
    "\n",
    "dataset = MusicDataset()\n",
    "train_dataset, val_dataset, test_dataset = stratify_split(\n",
    "    dataset, \n",
    "    dataset.classes,\n",
    "    transforms_waveform\n",
    ")\n",
    "\n",
    "by = \"waveform\"\n",
    "model_name = \"CEL2\"\n",
    "epochs = [50,100,300]   \n",
    "for i in range(len(epochs)):\n",
    "    train_model(model_name, epochs[i], device, hyperparameter_configs_CEL, dataset, train_dataset, val_dataset, n_input, by)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### by Spectogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limpiar caché de CUDA y recolectar basura\n",
    "torch.cuda.empty_cache()\n",
    "del dataset, train_dataset, val_dataset, test_dataset, trainloader, valloader, testloader\n",
    "gc.collect()  # Importante para liberar memoria RAM\n",
    "\n",
    "dataset = MusicDataset()\n",
    "train_dataset, val_dataset, test_dataset = stratify_split(\n",
    "    dataset, \n",
    "    dataset.classes,\n",
    "    transforms_spectrogram\n",
    ")\n",
    "\n",
    "by = \"spectrogram\"\n",
    "model_name = \"CEL2\"\n",
    "epochs = [50,100,300]   \n",
    "for i in range(len(epochs)):\n",
    "    train_model(model_name, epochs[i], device, hyperparameter_configs_CEL, dataset, train_dataset, val_dataset, n_input, by)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CEL3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### by Waveform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limpiar caché de CUDA y recolectar basura\n",
    "torch.cuda.empty_cache()\n",
    "del dataset, train_dataset, val_dataset, test_dataset, trainloader, valloader, testloader\n",
    "gc.collect()  # Importante para liberar memoria RAM\n",
    "\n",
    "dataset = MusicDataset()\n",
    "train_dataset, val_dataset, test_dataset = stratify_split(\n",
    "    dataset, \n",
    "    dataset.classes,\n",
    "    transforms_waveform\n",
    ")\n",
    "\n",
    "by = \"waveform\"\n",
    "model_name = \"CEL3\"\n",
    "epochs = [50,100,300]   \n",
    "for i in range(len(epochs)):\n",
    "    train_model(model_name, epochs[i], device, hyperparameter_configs_CEL, dataset, train_dataset, val_dataset, n_input, by)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### by Spectogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limpiar caché de CUDA y recolectar basura\n",
    "torch.cuda.empty_cache()\n",
    "del dataset, train_dataset, val_dataset, test_dataset, trainloader, valloader, testloader\n",
    "gc.collect()  # Importante para liberar memoria RAM\n",
    "\n",
    "dataset = MusicDataset()\n",
    "train_dataset, val_dataset, test_dataset = stratify_split(\n",
    "    dataset, \n",
    "    dataset.classes,\n",
    "    transforms_spectrogram\n",
    ")\n",
    "\n",
    "by = \"spectrogram\"\n",
    "model_name = \"CEL3\"\n",
    "epochs = [50,100,300]   \n",
    "for i in range(len(epochs)):\n",
    "    train_model(model_name, epochs[i], device, hyperparameter_configs_CEL, dataset, train_dataset, val_dataset, n_input, by)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TDVI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
